{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Perpetual","text":""},{"location":"#python-api-reference","title":"Python API Reference","text":"<p>The <code>PerpetualBooster</code> class is currently the only public facing class in the package, and can be used to train gradient boosted decision tree ensembles with multiple objective functions.</p>"},{"location":"#perpetual.PerpetualBooster","title":"PerpetualBooster","text":"<pre><code>PerpetualBooster(\n    *,\n    objective: Union[\n        str, Tuple[FunctionType, FunctionType, FunctionType]\n    ] = \"LogLoss\",\n    budget: float = 0.5,\n    num_threads: Optional[int] = None,\n    monotone_constraints: Union[\n        Dict[Any, int], None\n    ] = None,\n    force_children_to_bound_parent: bool = False,\n    missing: float = np.nan,\n    allow_missing_splits: bool = True,\n    create_missing_branch: bool = False,\n    terminate_missing_features: Optional[\n        Iterable[Any]\n    ] = None,\n    missing_node_treatment: str = \"None\",\n    log_iterations: int = 0,\n    feature_importance_method: str = \"Gain\",\n    quantile: Optional[float] = None,\n    reset: Optional[bool] = None,\n    categorical_features: Union[\n        Iterable[int], Iterable[str], str, None\n    ] = \"auto\",\n    timeout: Optional[float] = None,\n    iteration_limit: Optional[int] = None,\n    memory_limit: Optional[float] = None,\n    stopping_rounds: Optional[int] = None,\n    max_bin: int = 256,\n    max_cat: int = 1000\n)\n</code></pre> <p>PerpetualBooster class, used to create gradient boosted decision tree ensembles.</p> <p>Parameters:</p> <ul> <li> <code>objective</code>               (<code>str</code>, default:                   <code>'LogLoss'</code> )           \u2013            <p>Learning objective function to be used for optimization. Valid options are: \"LogLoss\" to use logistic loss (classification), \"SquaredLoss\" to use squared error (regression), \"QuantileLoss\" to use quantile error (regression), \"HuberLoss\" to use huber error (regression), \"AdaptiveHuberLoss\" to use adaptive huber error (regression). \"ListNetLoss\" to use ListNet loss (ranking). custom objective in the form of (grad, hess, init) where grad and hess are functions that take (y, pred, sample_weight, group) and return the gradient and hessian init is a function that takes (y, sample_weight, group) and returns the initial prediction value. Defaults to \"LogLoss\".</p> </li> <li> <code>budget</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>a positive number for fitting budget. Increasing this number will more likely result in more boosting rounds and more increased predictive power. Default value is 0.5.</p> </li> <li> <code>num_threads</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of threads to be used during training.</p> </li> <li> <code>monotone_constraints</code>               (<code>Dict[Any, int]</code>, default:                   <code>None</code> )           \u2013            <p>Constraints that are used to enforce a specific relationship between the training features and the target variable. A dictionary should be provided where the keys are the feature index value if the model will be fit on a numpy array, or a feature name if it will be fit on a Dataframe. The values of the dictionary should be an integer value of -1, 1, or 0 to specify the relationship that should be estimated between the respective feature and the target variable. Use a value of -1 to enforce a negative relationship, 1 a positive relationship, and 0 will enforce no specific relationship at all. Features not included in the mapping will not have any constraint applied. If <code>None</code> is passed, no constraints will be enforced on any variable. Defaults to <code>None</code>.</p> </li> <li> <code>force_children_to_bound_parent</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Setting this parameter to <code>True</code> will restrict children nodes, so that they always contain the parent node inside of their range. Without setting this it's possible that both, the left and the right nodes could be greater, than or less than, the parent node. Defaults to <code>False</code>.</p> </li> <li> <code>missing</code>               (<code>float</code>, default:                   <code>nan</code> )           \u2013            <p>Value to consider missing, when training and predicting with the booster. Defaults to <code>np.nan</code>.</p> </li> <li> <code>allow_missing_splits</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Allow for splits to be made such that all missing values go down one branch, and all non-missing values go down the other, if this results in the greatest reduction of loss. If this is false, splits will only be made on non missing values. If <code>create_missing_branch</code> is set to <code>True</code> having this parameter be set to <code>True</code> will result in the missing branch further split, if this parameter is <code>False</code> then in that case the missing branch will always be a terminal node. Defaults to <code>True</code>.</p> </li> <li> <code>create_missing_branch</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>An experimental parameter, that if <code>True</code>, will create a separate branch for missing, creating a ternary tree, the missing node will be given the same weight value as the parent node. If this parameter is <code>False</code>, missing will be sent down either the left or right branch, creating a binary tree. Defaults to <code>False</code>.</p> </li> <li> <code>terminate_missing_features</code>               (<code>Set[Any]</code>, default:                   <code>None</code> )           \u2013            <p>An optional iterable of features (either strings, or integer values specifying the feature indices if numpy arrays are used for fitting), for which the missing node will always be terminated, even if <code>allow_missing_splits</code> is set to true. This value is only valid if <code>create_missing_branch</code> is also True.</p> </li> <li> <code>missing_node_treatment</code>               (<code>str</code>, default:                   <code>'None'</code> )           \u2013            <p>Method for selecting the <code>weight</code> for the missing node, if <code>create_missing_branch</code> is set to <code>True</code>. Defaults to \"None\". Valid options are: - \"None\": Calculate missing node weight values without any constraints. - \"AssignToParent\": Assign the weight of the missing node to that of the parent. - \"AverageLeafWeight\": After training each tree, starting from the bottom of the tree, assign the missing node weight to the weighted average of the left and right child nodes. Next assign the parent to the weighted average of the children nodes. This is performed recursively up through the entire tree. This is performed as a post processing step on each tree after it is built, and prior to updating the predictions for which to train the next tree. - \"AverageNodeWeight\": Set the missing node to be equal to the weighted average weight of the left and the right nodes.</p> </li> <li> <code>log_iterations</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Setting to a value (N) other than zero will result in information being logged about ever N iterations, info can be interacted with directly with the python <code>logging</code> module. For an example of how to utilize the logging information see the example here.</p> </li> <li> <code>feature_importance_method</code>               (<code>str</code>, default:                   <code>'Gain'</code> )           \u2013            <p>The feature importance method type that will be used to calculate the <code>feature_importances_</code> attribute on the booster.</p> </li> <li> <code>quantile</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>only used in quantile regression.</p> </li> <li> <code>reset</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>whether to reset the model or continue training.</p> </li> <li> <code>categorical_features</code>               (<code>Union[Iterable[int], Iterable[str], str, None]</code>, default:                   <code>'auto'</code> )           \u2013            <p>The names or indices for categorical features. Defaults to <code>auto</code> for Polars or Pandas categorical data types.</p> </li> <li> <code>timeout</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>optional fit timeout in seconds</p> </li> <li> <code>iteration_limit</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>optional limit for the number of boosting rounds. The default value is 1000 boosting rounds. The algorithm automatically stops for most of the cases before hitting this limit. If you want to experiment with very high budget (&gt;2.0), you can also increase this limit.</p> </li> <li> <code>memory_limit</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>optional limit for memory allocation in GB. If not set, the memory will be allocated based on available memory and the algorithm requirements.</p> </li> <li> <code>stopping_rounds</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>optional limit for auto stopping.</p> </li> <li> <code>max_bin</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>maximum number of bins for feature discretization. Defaults to 256.</p> </li> <li> <code>max_cat</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum number of unique categories for a categorical feature. Features with more categories will be treated as numerical. Defaults to 1000.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>Raised if an invalid dtype is passed.</p> </li> </ul> Example <p>Once, the booster has been initialized, it can be fit on a provided dataset, and performance field. After fitting, the model can be used to predict on a dataset. In the case of this example, the predictions are the log odds of a given record being 1.</p> <pre><code># Small example dataset\nfrom seaborn import load_dataset\n\ndf = load_dataset(\"titanic\")\nX = df.select_dtypes(\"number\").drop(columns=[\"survived\"])\ny = df[\"survived\"]\n\n# Initialize a booster with defaults.\nfrom perpetual import PerpetualBooster\nmodel = PerpetualBooster(objective=\"LogLoss\")\nmodel.fit(X, y)\n\n# Predict on data\nmodel.predict(X.head())\n# array([-1.94919663,  2.25863229,  0.32963671,  2.48732194, -3.00371813])\n\n# predict contributions\nmodel.predict_contributions(X.head())\n# array([[-0.63014213,  0.33880048, -0.16520798, -0.07798772, -0.85083578,\n#        -1.07720813],\n#       [ 1.05406709,  0.08825999,  0.21662544, -0.12083538,  0.35209258,\n#        -1.07720813],\n</code></pre>"},{"location":"#perpetual.PerpetualBooster.base_score","title":"base_score  <code>property</code>","text":"<pre><code>base_score: Union[float, Iterable[float]]\n</code></pre> <p>Base score of the model.</p> <p>Returns:</p> <ul> <li> <code>Union[float, Iterable[float]]</code>           \u2013            <p>Union[float, Iterable[float]]: Base score(s) of the model.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.number_of_trees","title":"number_of_trees  <code>property</code>","text":"<pre><code>number_of_trees: Union[int, Iterable[int]]\n</code></pre> <p>The number of trees in the model.</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>Union[int, Iterable[int]]</code> )          \u2013            <p>The total number of trees in the model.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.fit","title":"fit","text":"<pre><code>fit(X, y, sample_weight=None, group=None) -&gt; Self\n</code></pre> <p>Fit the gradient booster on a provided dataset.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>FrameLike</code>)           \u2013            <p>Either a Polars or Pandas DataFrame, or a 2 dimensional Numpy array.</p> </li> <li> <code>y</code>               (<code>Union[FrameLike, ArrayLike]</code>)           \u2013            <p>Either a Polars or Pandas DataFrame or Series, or a 1 or 2 dimensional Numpy array.</p> </li> <li> <code>sample_weight</code>               (<code>Union[ArrayLike, None]</code>, default:                   <code>None</code> )           \u2013            <p>Instance weights to use when training the model. If None is passed, a weight of 1 will be used for every record. Defaults to None.</p> </li> <li> <code>group</code>               (<code>Union[ArrayLike, None]</code>, default:                   <code>None</code> )           \u2013            <p>Group lengths to use for a ranking objective. If None is passes, all items are assumed to be in the same group. Defaults to None.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.prune","title":"prune","text":"<pre><code>prune(X, y, sample_weight=None, group=None) -&gt; Self\n</code></pre> <p>Prune the gradient booster on a provided dataset.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>FrameLike</code>)           \u2013            <p>Either a Polars or Pandas DataFrame, or a 2 dimensional Numpy array.</p> </li> <li> <code>y</code>               (<code>Union[FrameLike, ArrayLike]</code>)           \u2013            <p>Either a Polars or Pandas DataFrame or Series, or a 1 or 2 dimensional Numpy array.</p> </li> <li> <code>sample_weight</code>               (<code>Union[ArrayLike, None]</code>, default:                   <code>None</code> )           \u2013            <p>Instance weights to use when training the model. If None is passed, a weight of 1 will be used for every record. Defaults to None.</p> </li> <li> <code>group</code>               (<code>Union[ArrayLike, None]</code>, default:                   <code>None</code> )           \u2013            <p>Group lengths to use for a ranking objective. If None is passes, all items are assumed to be in the same group. Defaults to None.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.calibrate","title":"calibrate","text":"<pre><code>calibrate(\n    X_train,\n    y_train,\n    X_cal,\n    y_cal,\n    alpha,\n    sample_weight=None,\n    group=None,\n) -&gt; Self\n</code></pre> <p>Calibrate the gradient booster on a provided dataset.</p> <p>Parameters:</p> <ul> <li> <code>X_train</code>               (<code>FrameLike</code>)           \u2013            <p>Either a Polars or Pandas DataFrame, or a 2 dimensional Numpy array.</p> </li> <li> <code>y_train</code>               (<code>Union[FrameLike, ArrayLike]</code>)           \u2013            <p>Either a Polars or Pandas DataFrame or Series, or a 1 or 2 dimensional Numpy array.</p> </li> <li> <code>X_cal</code>               (<code>FrameLike</code>)           \u2013            <p>Either a Polars or Pandas DataFrame, or a 2 dimensional Numpy array.</p> </li> <li> <code>y_cal</code>               (<code>Union[FrameLike, ArrayLike]</code>)           \u2013            <p>Either a Polars or Pandas DataFrame or Series, or a 1 or 2 dimensional Numpy array.</p> </li> <li> <code>alpha</code>               (<code>ArrayLike</code>)           \u2013            <p>Between 0 and 1, represents the uncertainty of the confidence interval. Lower alpha produce larger (more conservative) prediction intervals. alpha is the complement of the target coverage level.</p> </li> <li> <code>sample_weight</code>               (<code>Union[ArrayLike, None]</code>, default:                   <code>None</code> )           \u2013            <p>Instance weights to use when training the model. If None is passed, a weight of 1 will be used for every record. Defaults to None.</p> </li> <li> <code>group</code>               (<code>Union[ArrayLike, None]</code>, default:                   <code>None</code> )           \u2013            <p>Group lengths to use for a ranking objective. If None is passes, all items are assumed to be in the same group. Defaults to None.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.predict_intervals","title":"predict_intervals","text":"<pre><code>predict_intervals(\n    X, parallel: Union[bool, None] = None\n) -&gt; dict\n</code></pre> <p>Predict intervals with the fitted booster on new data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>FrameLike</code>)           \u2013            <p>Either a Polars or Pandas DataFrame, or a 2 dimensional Numpy array.</p> </li> <li> <code>parallel</code>               (<code>Union[bool, None]</code>, default:                   <code>None</code> )           \u2013            <p>Optionally specify if the predict function should run in parallel on multiple threads. If <code>None</code> is passed, the <code>parallel</code> attribute of the booster will be used. Defaults to <code>None</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>np.ndarray: Returns a numpy array of the predictions.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.predict","title":"predict","text":"<pre><code>predict(\n    X, parallel: Union[bool, None] = None\n) -&gt; np.ndarray\n</code></pre> <p>Predict with the fitted booster on new data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>FrameLike</code>)           \u2013            <p>Either a Polars or Pandas DataFrame, or a 2 dimensional Numpy array.</p> </li> <li> <code>parallel</code>               (<code>Union[bool, None]</code>, default:                   <code>None</code> )           \u2013            <p>Optionally specify if the predict function should run in parallel on multiple threads. If <code>None</code> is passed, the <code>parallel</code> attribute of the booster will be used. Defaults to <code>None</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Returns a numpy array of the predictions.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(\n    X, parallel: Union[bool, None] = None\n) -&gt; np.ndarray\n</code></pre> <p>Predict probabilities with the fitted booster on new data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>FrameLike</code>)           \u2013            <p>Either a Polars or Pandas DataFrame, or a 2 dimensional Numpy array.</p> </li> <li> <code>parallel</code>               (<code>Union[bool, None]</code>, default:                   <code>None</code> )           \u2013            <p>Optionally specify if the predict function should run in parallel on multiple threads. If <code>None</code> is passed, the <code>parallel</code> attribute of the booster will be used. Defaults to <code>None</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray, shape (n_samples, n_classes): Returns a numpy array of the class probabilities.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.predict_log_proba","title":"predict_log_proba","text":"<pre><code>predict_log_proba(\n    X, parallel: Union[bool, None] = None\n) -&gt; np.ndarray\n</code></pre> <p>Predict class log-probabilities with the fitted booster on new data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>FrameLike</code>)           \u2013            <p>Either a Polars or Pandas DataFrame, or a 2 dimensional Numpy array.</p> </li> <li> <code>parallel</code>               (<code>Union[bool, None]</code>, default:                   <code>None</code> )           \u2013            <p>Optionally specify if the predict function should run in parallel on multiple threads. If <code>None</code> is passed, the <code>parallel</code> attribute of the booster will be used. Defaults to <code>None</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Returns a numpy array of the predictions.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.predict_nodes","title":"predict_nodes","text":"<pre><code>predict_nodes(\n    X, parallel: Union[bool, None] = None\n) -&gt; List\n</code></pre> <p>Predict nodes with the fitted booster on new data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>FrameLike</code>)           \u2013            <p>Either a Polars or Pandas DataFrame, or a 2 dimensional Numpy array.</p> </li> <li> <code>parallel</code>               (<code>Union[bool, None]</code>, default:                   <code>None</code> )           \u2013            <p>Optionally specify if the predict function should run in parallel on multiple threads. If <code>None</code> is passed, the <code>parallel</code> attribute of the booster will be used. Defaults to <code>None</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List</code> (              <code>List</code> )          \u2013            <p>Returns a list of node predictions.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.predict_contributions","title":"predict_contributions","text":"<pre><code>predict_contributions(\n    X,\n    method: str = \"Average\",\n    parallel: Union[bool, None] = None,\n) -&gt; np.ndarray\n</code></pre> <p>Predict with the fitted booster on new data, returning the feature contribution matrix. The last column is the bias term.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>FrameLike</code>)           \u2013            <p>Either a pandas DataFrame, or a 2 dimensional numpy array.</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>'Average'</code> )           \u2013            <p>Method to calculate the contributions, available options are:</p> <ul> <li>\"Average\": If this option is specified, the average internal node values are calculated.</li> <li>\"Shapley\": Using this option will calculate contributions using the tree shap algorithm.</li> <li>\"Weight\": This method will use the internal leaf weights, to calculate the contributions. This is the same as what is described by Saabas here.</li> <li>\"BranchDifference\": This method will calculate contributions by subtracting the weight of the node the record will travel down by the weight of the other non-missing branch. This method does not have the property where the contributions summed is equal to the final prediction of the model.</li> <li>\"MidpointDifference\": This method will calculate contributions by subtracting the weight of the node the record will travel down by the mid-point between the right and left node weighted by the cover of each node. This method does not have the property where the contributions summed is equal to the final prediction of the model.</li> <li>\"ModeDifference\": This method will calculate contributions by subtracting the weight of the node the record will travel down by the weight of the node with the largest cover (the mode node). This method does not have the property where the contributions summed is equal to the final prediction of the model.</li> <li>\"ProbabilityChange\": This method is only valid when the objective type is set to \"LogLoss\". This method will calculate contributions as the change in a records probability of being 1 moving from a parent node to a child node. The sum of the returned contributions matrix, will be equal to the probability a record will be 1. For example, given a model, <code>model.predict_contributions(X, method=\"ProbabilityChange\") == 1 / (1 + np.exp(-model.predict(X)))</code></li> </ul> </li> <li> <code>parallel</code>               (<code>Union[bool, None]</code>, default:                   <code>None</code> )           \u2013            <p>Optionally specify if the predict function should run in parallel on multiple threads. If <code>None</code> is passed, the <code>parallel</code> attribute of the booster will be used. Defaults to <code>None</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Returns a numpy array of the predictions.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.partial_dependence","title":"partial_dependence","text":"<pre><code>partial_dependence(\n    X,\n    feature: Union[str, int],\n    samples: Optional[int] = 100,\n    exclude_missing: bool = True,\n    percentile_bounds: Tuple[float, float] = (0.2, 0.98),\n) -&gt; np.ndarray\n</code></pre> <p>Calculate the partial dependence values of a feature. For each unique value of the feature, this gives the estimate of the predicted value for that feature, with the effects of all features averaged out. This information gives an estimate of how a given feature impacts the model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>FrameLike</code>)           \u2013            <p>Either a pandas DataFrame, or a 2 dimensional numpy array. This should be the same data passed into the models fit, or predict, with the columns in the same order.</p> </li> <li> <code>feature</code>               (<code>Union[str, int]</code>)           \u2013            <p>The feature for which to calculate the partial dependence values. This can be the name of a column, if the provided X is a pandas DataFrame, or the index of the feature.</p> </li> <li> <code>samples</code>               (<code>Optional[int]</code>, default:                   <code>100</code> )           \u2013            <p>Number of evenly spaced samples to select. If None is passed all unique values will be used. Defaults to 100.</p> </li> <li> <code>exclude_missing</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Should missing excluded from the features? Defaults to True.</p> </li> <li> <code>percentile_bounds</code>               (<code>Tuple[float, float]</code>, default:                   <code>(0.2, 0.98)</code> )           \u2013            <p>Upper and lower percentiles to start at when calculating the samples. Defaults to (0.2, 0.98) to cap the samples selected at the 5th and 95th percentiles respectively.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>An error will be raised if the provided X parameter is not a pandas DataFrame, and a string is provided for the feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: A 2 dimensional numpy array, where the first column is the sorted unique values of the feature, and then the second column is the partial dependence values for each feature value.</p> </li> </ul> Example <p>This information can be plotted to visualize how a feature is used in the model, like so.</p> <p><pre><code>from seaborn import lineplot\nimport matplotlib.pyplot as plt\n\npd_values = model.partial_dependence(X=X, feature=\"age\", samples=None)\n\nfig = lineplot(x=pd_values[:,0], y=pd_values[:,1],)\nplt.title(\"Partial Dependence Plot\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Log Odds\")\n</code></pre> </p> <p>We can see how this is impacted if a model is created, where a specific constraint is applied to the feature using the <code>monotone_constraint</code> parameter.</p> <p><pre><code>model = PerpetualBooster(\n    objective=\"LogLoss\",\n    monotone_constraints={\"age\": -1},\n)\nmodel.fit(X, y)\n\npd_values = model.partial_dependence(X=X, feature=\"age\")\nfig = lineplot(\n    x=pd_values[:, 0],\n    y=pd_values[:, 1],\n)\nplt.title(\"Partial Dependence Plot with Monotonicity\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Log Odds\")\n</code></pre> </p>"},{"location":"#perpetual.PerpetualBooster.calculate_feature_importance","title":"calculate_feature_importance","text":"<pre><code>calculate_feature_importance(\n    method: str = \"Gain\", normalize: bool = True\n) -&gt; Union[Dict[int, float], Dict[str, float]]\n</code></pre> <p>Feature importance values can be calculated with the <code>calculate_feature_importance</code> method. This function will return a dictionary of the features and their importance values. It should be noted that if a feature was never used for splitting it will not be returned in importance dictionary.</p> <p>Parameters:</p> <ul> <li> <code>method</code>               (<code>str</code>, default:                   <code>'Gain'</code> )           \u2013            <p>Variable importance method. Defaults to \"Gain\". Valid options are:</p> <ul> <li>\"Weight\": The number of times a feature is used to split the data across all trees.</li> <li>\"Gain\": The average split gain across all splits the feature is used in.</li> <li>\"Cover\": The average coverage across all splits the feature is used in.</li> <li>\"TotalGain\": The total gain across all splits the feature is used in.</li> <li>\"TotalCover\": The total coverage across all splits the feature is used in.</li> </ul> </li> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Should the importance be normalized to sum to 1? Defaults to <code>True</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Dict[int, float], Dict[str, float]]</code>           \u2013            <p>Dict[str, float]: Variable importance values, for features present in the model.</p> </li> </ul> Example <pre><code>model.calculate_feature_importance(\"Gain\")\n# {\n#   'parch': 0.0713072270154953,\n#   'age': 0.11609109491109848,\n#   'sibsp': 0.1486879289150238,\n#   'fare': 0.14309120178222656,\n#   'pclass': 0.5208225250244141\n# }\n</code></pre>"},{"location":"#perpetual.PerpetualBooster.text_dump","title":"text_dump","text":"<pre><code>text_dump() -&gt; List[str]\n</code></pre> <p>Return all of the trees of the model in text form.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of strings, where each string is a text representation of the tree.</p> </li> </ul> <p>Example:     <pre><code>model.text_dump()[0]\n# 0:[0 &lt; 3] yes=1,no=2,missing=2,gain=91.50833,cover=209.388307\n#       1:[4 &lt; 13.7917] yes=3,no=4,missing=4,gain=28.185467,cover=94.00148\n#             3:[1 &lt; 18] yes=7,no=8,missing=8,gain=1.4576768,cover=22.090348\n#                   7:[1 &lt; 17] yes=15,no=16,missing=16,gain=0.691266,cover=0.705011\n#                         15:leaf=-0.15120,cover=0.23500\n#                         16:leaf=0.154097,cover=0.470007\n</code></pre></p>"},{"location":"#perpetual.PerpetualBooster.json_dump","title":"json_dump","text":"<pre><code>json_dump() -&gt; str\n</code></pre> <p>Return the booster object as a string.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The booster dumped as a json object in string form.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.load_booster","title":"load_booster  <code>classmethod</code>","text":"<pre><code>load_booster(path: str) -&gt; Self\n</code></pre> <p>Load a booster object that was saved with the <code>save_booster</code> method.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved booster file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerpetualBooster</code> (              <code>Self</code> )          \u2013            <p>An initialized booster object.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.save_booster","title":"save_booster","text":"<pre><code>save_booster(path: str)\n</code></pre> <p>Save a booster object, the underlying representation is a json file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to save the booster object.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.insert_metadata","title":"insert_metadata","text":"<pre><code>insert_metadata(key: str, value: str)\n</code></pre> <p>Insert data into the models metadata, this will be saved on the booster object.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>Key to give the inserted value in the metadata.</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>String value to assign to the key.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata(key: str) -&gt; str\n</code></pre> <p>Get the value associated with a given key, on the boosters metadata.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>Key of item in metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Value associated with the provided key in the boosters metadata.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.get_params","title":"get_params","text":"<pre><code>get_params(deep=True) -&gt; Dict[str, Any]\n</code></pre> <p>Get all of the parameters for the booster.</p> <p>Parameters:</p> <ul> <li> <code>deep</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>This argument does nothing, and is simply here for scikit-learn compatibility.. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dict[str, Any]: The parameters of the booster.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.set_params","title":"set_params","text":"<pre><code>set_params(**params: Any) -&gt; Self\n</code></pre> <p>Set the parameters of the booster, this has the same effect as reinstating the booster.</p> <p>Returns:</p> <ul> <li> <code>PerpetualBooster</code> (              <code>Self</code> )          \u2013            <p>Booster with new parameters.</p> </li> </ul>"},{"location":"#perpetual.PerpetualBooster.get_node_lists","title":"get_node_lists","text":"<pre><code>get_node_lists(\n    map_features_names: bool = True,\n) -&gt; List[List[Node]]\n</code></pre> <p>Return the tree structures representation as a list of python objects.</p> <p>Parameters:</p> <ul> <li> <code>map_features_names</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Should the feature names tried to be mapped to a string, if a pandas dataframe was used. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[List[Node]]</code>           \u2013            <p>List[List[Node]]: A list of lists where each sub list is a tree, with all of it's respective nodes.</p> </li> </ul> Example <p>This can be run directly to get the tree structure as python objects.</p> <pre><code>model = PerpetualBooster()\nmodel.fit(X, y)\n\nmodel.get_node_lists()[0]\n\n# [Node(num=0, weight_value...,\n# Node(num=1, weight_value...,\n# Node(num=2, weight_value...,\n# Node(num=3, weight_value...,\n# Node(num=4, weight_value...,\n# Node(num=5, weight_value...,\n# Node(num=6, weight_value...,]\n</code></pre>"},{"location":"#perpetual.PerpetualBooster.trees_to_dataframe","title":"trees_to_dataframe","text":"<pre><code>trees_to_dataframe()\n</code></pre> <p>Return the tree structure as a Polars or Pandas DataFrame object.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>          \u2013            <p>Trees in a Polars or Pandas DataFrame.</p> </li> </ul> Example <p>This can be used directly to print out the tree structure as a dataframe. The Leaf values will have the \"Gain\" column replaced with the weight value.</p> <pre><code>model.trees_to_dataframe().head()\n</code></pre> Tree Node ID Feature Split Yes No Missing Gain Cover 0 0 0 0-0 pclass 3 0-1 0-2 0-2 91.5083 209.388 1 0 1 0-1 fare 13.7917 0-3 0-4 0-4 28.1855 94.0015"},{"location":"#logging-output","title":"Logging output","text":"<p>Info is logged while the model is being trained if the <code>log_iterations</code> parameter is set to a value greater than <code>0</code> while fitting the booster. The logs can be printed to stdout while training like so.</p> <pre><code>import logging\nlogging.basicConfig()\nlogging.getLogger().setLevel(logging.INFO)\n\nmodel = PerpetualBooster(log_iterations=1)\nmodel.fit(X, y)\n\n# INFO:perpetual.perpetualbooster:Completed iteration 0 of 10\n# INFO:perpetual.perpetualbooster:Completed iteration 1 of 10\n# INFO:perpetual.perpetualbooster:Completed iteration 2 of 10\n</code></pre> <p>The log output can also be captured in a file also using the <code>logging.basicConfig()</code> <code>filename</code> option.</p> <pre><code>import logging\nlogging.basicConfig(filename=\"training-info.log\")\nlogging.getLogger().setLevel(logging.INFO)\n\nmodel = PerpetualBooster(log_iterations=10)\nmodel.fit(X, y)\n</code></pre>"}]}