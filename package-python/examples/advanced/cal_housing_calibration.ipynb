{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration - Predicting Intervals with PerpetualBooster\n",
    "\n",
    "This notebook explores different methods for producing prediction intervals with `PerpetualBooster`. \n",
    "We compare several Perpetual-native methods that leverage internal leaf weight statistics against the popular MAPIE conformal prediction library.\n",
    "\n",
    "### Methods Evaluated:\n",
    "1.  **Perpetual Method 1 (Min-Max)**: Range-based intervals using leaf fold weights.\n",
    "2.  **Perpetual Method 2 (Global Relative Position)**: Quantile-based intervals using piecewise interpolation of all fold weights.\n",
    "3.  **Perpetual Method 3 (Weight Variance)**: Variance-based intervals using the standard deviation of local fold weights.\n",
    "4.  **Perpetual Method 4 (Native `calibrate`)**: The built-in Rust implementation for prediction intervals.\n",
    "5.  **MAPIE Split Conformal**: Constant-width model-agnostic conformal prediction.\n",
    "6.  **MAPIE CV+**: Improved conformal prediction using cross-validation (Jackknife+).\n",
    "7.  **MAPIE CQR**: Conformal Quantile Regression using quantile-loss baseline models.\n",
    "\n",
    "We will evaluate these methods on the California Housing dataset, targeting a 90% coverage level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from perpetual import PerpetualBooster\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Splitting\n",
    "\n",
    "We load the California Housing dataset using `sklearn`. \n",
    "*   **Train**: Used to train the `PerpetualBooster` model.\n",
    "*   **Calibration**: Used to learn the mapping from model outputs to actual probabilities/coverage.\n",
    "*   **Test**: Used to evaluate the final performance.\n",
    "\n",
    "We use a 60/20/20 split for Train/Calibration/Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into Test (20%) and Rest (80%)\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split Rest into Train (75% of Rest) and Calibration (25% of Rest)\n",
    "X_train, X_cal, y_train, y_cal = train_test_split(\n",
    "    X_rest, y_rest, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Calibration shape: {X_cal.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "We train the model with `save_node_stats=True`. This is critical as it instructs the booster to persist the summary statistics (min, max, etc.) of the gradients/hessians (or target values) in each leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PerpetualBooster\n",
    "# Objective: SquaredLoss (Regression)\n",
    "model = PerpetualBooster(objective=\"SquaredLoss\", save_node_stats=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Number of trees: {len(model.get_node_lists())}\")\n",
    "print(f\"Base Score: {model.base_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Method 1: Min-Max Interval\n",
    "\n",
    "### Logic\n",
    "Each leaf node $l$ in tree $t$ has a range of weights $[w_{t,l}^{min}, w_{t,l}^{max}]$.\n",
    "For a given input sample $x$, let $L_t(x)$ be the leaf it falls into in tree $t$.\n",
    "\n",
    "We define the **Ensemble Minimum** ($S_{min}$) and **Ensemble Maximum** ($S_{max}$) as:\n",
    "$$ S_{min}(x) = \\sum_{t=1}^T w_{t, L_t(x)}^{min} + \\text{base\\_score} $$\n",
    "$$ S_{max}(x) = \\sum_{t=1}^T w_{t, L_t(x)}^{max} + \\text{base\\_score} $$\n",
    "\n",
    "These define a \"hard\" interval $[S_{min}, S_{max}]$. However, this interval might be too wide or too narrow (uncalibrated).\n",
    "We calculate the **Relative Position** ($P_{rel}$) of the true label $y$:\n",
    "$$ P_{rel} = \\frac{y - S_{min}}{S_{max} - S_{min}} $$\n",
    "\n",
    "If the model is perfectly calibrated in a naive min-max sense, $P_{rel}$ might be Uniform(0,1). Usually, it is not. By looking at the distribution of $P_{rel}$ on the calibration set, we can map any desired confidence level (e.g., 90%) to specific values of $P_{rel}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaf_nodes_and_predict(model, X):\n",
    "    # Get the raw visited nodes for each sample\n",
    "    # This return shape (n_trees, n_samples) and each element is a set of nodes traversed\n",
    "    pred_nodes = model.predict_nodes(X)\n",
    "\n",
    "    # Get all leaf nodes from the model structure\n",
    "    tree_nodes_list = model.get_node_lists()\n",
    "\n",
    "    # Create a fast lookup for leaf weights: tree_idx -> node_id -> weights\n",
    "    leaf_weights_lookup = []\n",
    "    for tree_nodes in tree_nodes_list:\n",
    "        lookup = {node.num: node.weights for node in tree_nodes if node.is_leaf}\n",
    "        leaf_weights_lookup.append(lookup)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_trees = len(pred_nodes)\n",
    "\n",
    "    # Array to store min and max for each tree prediction\n",
    "    tree_min_max = np.zeros((n_samples, n_trees, 2))\n",
    "\n",
    "    for t in range(n_trees):\n",
    "        for i in range(n_samples):\n",
    "            # Find the leaf node ID in the visited set\n",
    "            visited_nodes = pred_nodes[t][i]\n",
    "            # The leaf node is the only one in the set that is present in the leaf_weights_lookup\n",
    "            leaf_id = next(\n",
    "                nid for nid in visited_nodes if nid in leaf_weights_lookup[t]\n",
    "            )\n",
    "\n",
    "            weights = leaf_weights_lookup[t][leaf_id]\n",
    "            tree_min_max[i, t, 0] = min(weights)\n",
    "            tree_min_max[i, t, 1] = max(weights)\n",
    "\n",
    "    # Sum across trees\n",
    "    s_min = np.sum(tree_min_max[:, :, 0], axis=1) + model.base_score\n",
    "    s_max = np.sum(tree_min_max[:, :, 1], axis=1) + model.base_score\n",
    "\n",
    "    return s_min, s_max\n",
    "\n",
    "\n",
    "# Calculate intervals for Calibration Set\n",
    "cal_s_min, cal_s_max = get_leaf_nodes_and_predict(model, X_cal)\n",
    "\n",
    "# Check coverage of the raw interval [S_min, S_max]\n",
    "raw_coverage = np.mean((y_cal >= cal_s_min) & (y_cal <= cal_s_max))\n",
    "print(f\"Raw Interval [S_min, S_max] Coverage on Calibration Set: {raw_coverage:.4f}\")\n",
    "\n",
    "# Calculate Relative Position (P_rel)\n",
    "denom = cal_s_max - cal_s_min\n",
    "min_valid_denom = 1e-6\n",
    "denom[denom < min_valid_denom] = min_valid_denom  # Avoid division by zero\n",
    "\n",
    "cal_p_rel = (y_cal - cal_s_min) / denom\n",
    "\n",
    "# Visualize P_rel distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(cal_p_rel, bins=100, kde=True)\n",
    "plt.title(\"Distribution of Relative Position (P_rel) on Calibration Set\")\n",
    "plt.xlabel(\"Relative Position within [Min, Max] Interval (0=Min, 1=Max)\")\n",
    "plt.axvline(0, color=\"r\", linestyle=\"--\")\n",
    "plt.axvline(1, color=\"r\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Mapping\n",
    "\n",
    "We use the empirical quantiles of $P_{rel}$ to define our calibrated intervals.\n",
    "To get a $90\\%$ confidence interval (covering from 5% to 95% of probability mass), we find the $5^{th}$ and $95^{th}$ percentiles of the $P_{rel}$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibrated_interval_m1(s_min, s_max, cal_p_rel_dist, confidence_level=0.90):\n",
    "    alpha = 1.0 - confidence_level\n",
    "    target_lower = alpha / 2.0\n",
    "    target_upper = 1.0 - (alpha / 2.0)\n",
    "\n",
    "    # Find the threshold values of P_rel that correspond to these quantiles\n",
    "    p_rel_lower = np.quantile(cal_p_rel_dist, target_lower)\n",
    "    p_rel_upper = np.quantile(cal_p_rel_dist, target_upper)\n",
    "\n",
    "    # Apply these relative thresholds to the specific S_min/S_max of each sample\n",
    "    pred_lower = s_min + p_rel_lower * (s_max - s_min)\n",
    "    pred_upper = s_min + p_rel_upper * (s_max - s_min)\n",
    "\n",
    "    return pred_lower, pred_upper\n",
    "\n",
    "\n",
    "# Apply to Test Set\n",
    "test_s_min, test_s_max = get_leaf_nodes_and_predict(model, X_test)\n",
    "\n",
    "# Calculate 90% Interval\n",
    "test_m1_lower, test_m1_upper = get_calibrated_interval_m1(\n",
    "    test_s_min, test_s_max, cal_p_rel, 0.90\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "m1_coverage = np.mean((y_test >= test_m1_lower) & (y_test <= test_m1_upper))\n",
    "m1_width = np.mean(test_m1_upper - test_m1_lower)\n",
    "\n",
    "print(\"Method 1 (Min-Max) Evaluation on Test Set (Target 90%):\")\n",
    "print(f\"Coverage: {m1_coverage:.4f}\")\n",
    "print(f\"Mean Width: {m1_width:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Method 2: Global Relative Position\n",
    "\n",
    "### Logic: Consistent Ensemble Interpolation\n",
    "A naive simulation that randomly picks one weight per tree independently will suffer from **Variance Collapse** due to the Central Limit Theorem.\n",
    "\n",
    "To fix this, we use **Consistent Ensemble Interpolation**:\n",
    "1. **Sort Weights**: The 5 leaf weights are cross-validation fold weights that are **not inherently ordered**. We apply `np.sort` to treat them as an ordered 5-number summary.\n",
    "2. **Aggregate First**: For each sample, we sum the sorted 5-number summary weights across all trees. This gives us an **Ensemble 5-Number Summary**.\n",
    "\n",
    "### Global Relative Position Calibration (with Extrapolation)\n",
    "The key insight is that the ensemble's 5-number summary range only covers ~48% of the data. To achieve 90% coverage, we need to **extrapolate** beyond the raw min/max.\n",
    "\n",
    "For each sample on the calibration set, we compute a **quantile position** $P$ of the true value within the ensemble distribution, allowing $P < 0$ or $P > 1$ via linear extrapolation beyond the endpoints.\n",
    "\n",
    "We then take the 5th and 95th percentiles of the $P$ distribution on the calibration set, and use these to construct calibrated intervals on the test set by interpolating/extrapolating the ensemble distribution at those target positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_summary(model, X):\n",
    "    \"\"\"Get the ensemble 5-number summary for each sample.\"\"\"\n",
    "    pred_nodes = model.predict_nodes(X)\n",
    "    tree_nodes_list = model.get_node_lists()\n",
    "\n",
    "    leaf_weights_lookup = []\n",
    "    for tree_nodes in tree_nodes_list:\n",
    "        # CRITICAL: Sort the weights. They are CV fold weights, NOT inherently ordered.\n",
    "        lookup = {\n",
    "            node.num: np.sort(np.array(node.weights))\n",
    "            for node in tree_nodes\n",
    "            if node.is_leaf\n",
    "        }\n",
    "        leaf_weights_lookup.append(lookup)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_trees = len(pred_nodes)\n",
    "    ensemble_stats = np.zeros((n_samples, 5))\n",
    "\n",
    "    for t in range(n_trees):\n",
    "        lookup = leaf_weights_lookup[t]\n",
    "        for i in range(n_samples):\n",
    "            leaf_id = next(nid for nid in pred_nodes[t][i] if nid in lookup)\n",
    "            ensemble_stats[i, :] += lookup[leaf_id]\n",
    "\n",
    "    ensemble_stats += model.base_score\n",
    "    return ensemble_stats\n",
    "\n",
    "\n",
    "def get_quantile_position(ensemble_stats, y_true):\n",
    "    \"\"\"Compute the Global Relative Position of the truth within the ensemble distribution.\n",
    "    Supports extrapolation: P < 0 means below min, P > 1 means above max.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    stat_q = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "    n_samples = len(y_true)\n",
    "    positions = np.zeros(n_samples)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        vals = ensemble_stats[i, :]  # sorted 5-number summary\n",
    "        y = y_true[i]\n",
    "\n",
    "        if y <= vals[0]:\n",
    "            # Extrapolate below min using slope of first segment\n",
    "            delta = vals[1] - vals[0]\n",
    "            if delta > 1e-12:\n",
    "                positions[i] = (y - vals[0]) / (delta / (stat_q[1] - stat_q[0]))\n",
    "            else:\n",
    "                positions[i] = 0.0\n",
    "        elif y >= vals[-1]:\n",
    "            # Extrapolate above max using slope of last segment\n",
    "            delta = vals[-1] - vals[-2]\n",
    "            if delta > 1e-12:\n",
    "                positions[i] = 1.0 + (y - vals[-1]) / (\n",
    "                    delta / (stat_q[-1] - stat_q[-2])\n",
    "                )\n",
    "            else:\n",
    "                positions[i] = 1.0\n",
    "        else:\n",
    "            # Interpolate within the distribution\n",
    "            for k in range(4):\n",
    "                if vals[k] <= y <= vals[k + 1]:\n",
    "                    delta = vals[k + 1] - vals[k]\n",
    "                    if delta > 1e-12:\n",
    "                        frac = (y - vals[k]) / delta\n",
    "                    else:\n",
    "                        frac = 0.5\n",
    "                    positions[i] = stat_q[k] + frac * (stat_q[k + 1] - stat_q[k])\n",
    "                    break\n",
    "    return positions\n",
    "\n",
    "\n",
    "def get_calibrated_interval_m2(ensemble_stats, cal_positions, confidence_level=0.9):\n",
    "    \"\"\"Build calibrated intervals by extrapolating the ensemble distribution.\"\"\"\n",
    "    alpha = 1.0 - confidence_level\n",
    "    p_low = np.quantile(cal_positions, alpha / 2.0)\n",
    "    p_high = np.quantile(cal_positions, 1.0 - alpha / 2.0)\n",
    "\n",
    "    stat_q = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "    n_samples = ensemble_stats.shape[0]\n",
    "    lower = np.zeros(n_samples)\n",
    "    upper = np.zeros(n_samples)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        vals = ensemble_stats[i, :]\n",
    "        # Interpolate within [0, 1]\n",
    "        lower[i] = np.interp(p_low, stat_q, vals)\n",
    "        upper[i] = np.interp(p_high, stat_q, vals)\n",
    "\n",
    "        # Handle extrapolation beyond [0, 1]\n",
    "        if p_low < 0:\n",
    "            slope = (vals[1] - vals[0]) / (stat_q[1] - stat_q[0])\n",
    "            lower[i] = vals[0] + slope * p_low\n",
    "        if p_high > 1:\n",
    "            slope = (vals[-1] - vals[-2]) / (stat_q[-1] - stat_q[-2])\n",
    "            upper[i] = vals[-1] + slope * (p_high - 1.0)\n",
    "\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "print(\"Calculating Ensemble Summaries (with sorted weights)...\")\n",
    "cal_ensemble_stats = get_ensemble_summary(model, X_cal)\n",
    "\n",
    "# Compute Global Relative Position on calibration set\n",
    "cal_positions = get_quantile_position(cal_ensemble_stats, y_cal)\n",
    "print(\n",
    "    f\"Calibration Positions: min={cal_positions.min():.4f}, max={cal_positions.max():.4f}, mean={cal_positions.mean():.4f}\"\n",
    ")\n",
    "\n",
    "# Compute calibrated interval on calibration set for sanity check\n",
    "m2_cal_lower, m2_cal_upper = get_calibrated_interval_m2(\n",
    "    cal_ensemble_stats, cal_positions\n",
    ")\n",
    "m2_coverage = np.mean((y_cal >= m2_cal_lower) & (y_cal <= m2_cal_upper))\n",
    "print(f\"Method 2 Calibration Coverage on Calibration Set: {m2_coverage:.4f}\")\n",
    "\n",
    "# Apply to Test Set\n",
    "test_m2_ensemble_stats = get_ensemble_summary(model, X_test)\n",
    "m2_lower, m2_upper = get_calibrated_interval_m2(test_m2_ensemble_stats, cal_positions)\n",
    "\n",
    "m2_coverage = np.mean((y_test >= m2_lower) & (y_test <= m2_upper))\n",
    "m2_width = np.mean(m2_upper - m2_lower)\n",
    "\n",
    "print(\"Method 2 (GRP) Evaluation on Test Set:\")\n",
    "print(f\"Coverage: {m2_coverage:.4f}\")\n",
    "print(f\"Mean Width: {m2_width:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method 3: Weight Variance Method\n",
    "\n",
    "This method leverages the standard deviation of leaf weights across the 5 cross-validation folds. \n",
    "The interval width is assumed to be proportional to the sum of standard deviations across all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_variance_uncertainty(model, X):\n",
    "    pred_nodes = model.predict_nodes(X)\n",
    "    tree_nodes_list = model.get_node_lists()\n",
    "\n",
    "    leaf_std_lookup = []\n",
    "    for tree_nodes in tree_nodes_list:\n",
    "        # Calculate std deviation of weights for each leaf node\n",
    "        lookup = {node.num: np.std(node.weights) for node in tree_nodes if node.is_leaf}\n",
    "        leaf_std_lookup.append(lookup)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_trees = len(pred_nodes)\n",
    "\n",
    "    # Aggregate uncertainty across trees\n",
    "    uncertainty = np.zeros(n_samples)\n",
    "    for t in range(n_trees):\n",
    "        current_lookup = leaf_std_lookup[t]\n",
    "        for i in range(n_samples):\n",
    "            visited = pred_nodes[t][i]\n",
    "            leaf_id = next(nid for nid in visited if nid in current_lookup)\n",
    "            uncertainty[i] += current_lookup[leaf_id]\n",
    "\n",
    "    return uncertainty\n",
    "\n",
    "\n",
    "# Calculate uncertainty on calibration set\n",
    "cal_uncertainty = get_weight_variance_uncertainty(model, X_cal)\n",
    "\n",
    "# Calculate residuals on calibration set\n",
    "cal_preds = model.predict(X_cal)\n",
    "if isinstance(cal_preds, tuple):\n",
    "    cal_preds = cal_preds[0]\n",
    "abs_residuals = np.abs(y_cal - cal_preds)\n",
    "\n",
    "# Calibrate scaling factor: residuals / uncertainty\n",
    "scaling_factors = abs_residuals / np.maximum(cal_uncertainty, 1e-6)\n",
    "q_factor = np.quantile(scaling_factors, 0.9)  # for 90% coverage\n",
    "\n",
    "print(f\"Weight Variance Scaling Factor (90% quantile): {q_factor:.4f}\")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "test_uncertainty = get_weight_variance_uncertainty(model, X_test)\n",
    "test_preds = model.predict(X_test)\n",
    "if isinstance(test_preds, tuple):\n",
    "    test_preds = test_preds[0]\n",
    "\n",
    "m3_wv_lower = test_preds - q_factor * test_uncertainty\n",
    "m3_wv_upper = test_preds + q_factor * test_uncertainty\n",
    "\n",
    "m3_wv_coverage = np.mean((y_test >= m3_wv_lower) & (y_test <= m3_wv_upper))\n",
    "m3_wv_width = np.mean(m3_wv_upper - m3_wv_lower)\n",
    "\n",
    "print(\"Method 3 (Weight Variance) Evaluation on Test Set:\")\n",
    "print(f\"Coverage: {m3_wv_coverage:.4f}\")\n",
    "print(f\"Mean Width: {m3_wv_width:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 4: Native Calibration\n",
    "\n",
    "PerpetualBooster provides a native `calibrate` method that automates the calibration process using the same cross-validation fold weights inside the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Native Calibrate\n",
    "# We use a significance level alpha=0.1 for 90% coverage\n",
    "model.calibrate_conformal(X_train, y_train, X_cal, y_cal, alpha=np.array([0.1]))\n",
    "\n",
    "# Native prediction returns a dictionary mapping alpha to (lower, upper)\n",
    "intervals = model.predict_intervals(X_test)\n",
    "m3_lower, m3_upper = intervals[\"0.1\"][:, 0], intervals[\"0.1\"][:, 1]\n",
    "\n",
    "m3_coverage = np.mean((y_test >= m3_lower) & (y_test <= m3_upper))\n",
    "m3_width = np.mean(m3_upper - m3_lower)\n",
    "\n",
    "print(\"Method 4 (Native Calibrate) Evaluation on Test Set (Target 90%):\")\n",
    "print(f\"Coverage: {m3_coverage:.4f}\")\n",
    "print(f\"Mean Width: {m3_width:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional MAPIE Methods\n",
    "\n",
    "Beyond Split Conformal, MAPIE supports more advanced methods like CV+ (Jackknife+) and Conformal Quantile Regression (CQR).\n",
    "- **CV+ / Jackknife+**: Uses cross-validation to get more stable intervals, often better for smaller datasets.\n",
    "- **CQR**: Calibrates quantile regression models, producing adaptive-width intervals based on estimated quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapie.regression import ConformalizedQuantileRegressor, CrossConformalRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "# MAPIE CV+ (Jackknife+)\n",
    "# We use cv=5 to match Perpetual's internal fold count\n",
    "mapie_cv_plus = CrossConformalRegressor(\n",
    "    estimator=GradientBoostingRegressor(random_state=42), cv=5, method=\"plus\"\n",
    ")\n",
    "mapie_cv_plus.fit_conformalize(X_rest, y_rest)\n",
    "_, y_pis_cv = mapie_cv_plus.predict_interval(X_test)\n",
    "m_cv_lower, m_cv_upper = y_pis_cv[:, 0, 0], y_pis_cv[:, 1, 0]\n",
    "\n",
    "m_cv_coverage = np.mean((y_test >= m_cv_lower) & (y_test <= m_cv_upper))\n",
    "m_cv_width = np.mean(m_cv_upper - m_cv_lower)\n",
    "print(f\"MAPIE CV+ Coverage: {m_cv_coverage:.4f}, Mean Width: {m_cv_width:.4f}\")\n",
    "\n",
    "# MAPIE CQR (Conformal Quantile Regression)\n",
    "est_cqr = HistGradientBoostingRegressor(loss=\"quantile\", quantile=0.5, random_state=42)\n",
    "mapie_cqr = ConformalizedQuantileRegressor(est_cqr)\n",
    "mapie_cqr.fit(X_train, y_train)\n",
    "mapie_cqr.conformalize(X_cal, y_cal)\n",
    "_, y_pis_cqr = mapie_cqr.predict_interval(X_test)\n",
    "m_cqr_lower, m_cqr_upper = y_pis_cqr[:, 0, 0], y_pis_cqr[:, 1, 0]\n",
    "\n",
    "m_cqr_coverage = np.mean((y_test >= m_cqr_lower) & (y_test <= m_cqr_upper))\n",
    "m_cqr_width = np.mean(m_cqr_upper - m_cqr_lower)\n",
    "print(f\"MAPIE CQR Coverage: {m_cqr_coverage:.4f}, Mean Width: {m_cqr_width:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison and Results\n",
    "\n",
    "We compare Method 1 and Method 2 on the Test Set. Both methods utilize the internal statistics of the booster and achieve approximately 90% calibrated coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline: MAPIE (Split Conformal)\n",
    "\n",
    "We compare against [MAPIE](https://mapie.readthedocs.io/), a popular conformal prediction library.\n",
    "MAPIE's `SplitConformalRegressor` uses a model-agnostic approach: it fits any sklearn regressor,\n",
    "computes residuals on a calibration set, and uses their quantiles to build prediction intervals.\n",
    "\n",
    "Key difference: MAPIE produces **constant-width** intervals (same absolute width for all predictions),\n",
    "while our methods produce **adaptive-width** intervals that vary based on each sample's leaf weight spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapie.regression import SplitConformalRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Train a GradientBoostingRegressor (same family as PerpetualBooster)\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# MAPIE Split Conformal: calibrate residuals on calibration set\n",
    "mapie = SplitConformalRegressor(estimator=gbr, confidence_level=0.9, prefit=True)\n",
    "mapie.conformalize(X_cal, y_cal)\n",
    "\n",
    "# Predict intervals on test set\n",
    "y_pred_mapie, y_pis = mapie.predict_interval(X_test)\n",
    "mapie_lower = y_pis[:, 0, 0]\n",
    "mapie_upper = y_pis[:, 1, 0]\n",
    "\n",
    "mapie_coverage = np.mean((y_test >= mapie_lower) & (y_test <= mapie_upper))\n",
    "mapie_width = np.mean(mapie_upper - mapie_lower)\n",
    "print(f\"MAPIE Coverage: {mapie_coverage:.4f}\")\n",
    "print(f\"MAPIE Mean Width: {mapie_width:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison: interval widths across key methods\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 10), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "sort_idx = np.argsort(\n",
    "    model.predict(X_test)[0]\n",
    "    if isinstance(model.predict(X_test), tuple)\n",
    "    else model.predict(X_test)\n",
    ")\n",
    "x_range = np.arange(len(sort_idx))\n",
    "y_test_arr = np.array(y_test)\n",
    "\n",
    "methods = [\n",
    "    (\"MAPIE Split\", mapie_lower, mapie_upper, mapie_coverage, mapie_width, \"gray\"),\n",
    "    (\"MAPIE CV+\", m_cv_lower, m_cv_upper, m_cv_coverage, m_cv_width, \"purple\"),\n",
    "    (\"MAPIE CQR\", m_cqr_lower, m_cqr_upper, m_cqr_coverage, m_cqr_width, \"brown\"),\n",
    "    (\"Perpetual M1\", test_m1_lower, test_m1_upper, m1_coverage, m1_width, \"green\"),\n",
    "    (\"Perpetual M2\", m2_lower, m2_upper, m2_coverage, m2_width, \"orange\"),\n",
    "    (\"Perpetual M3 (WV)\", m3_wv_lower, m3_wv_upper, m3_wv_coverage, m3_wv_width, \"red\"),\n",
    "    (\"Perpetual M4 (Nat)\", m3_lower, m3_upper, m3_coverage, m3_width, \"blue\"),\n",
    "]\n",
    "\n",
    "for i, (name, low, high, cov, wid, col) in enumerate(methods):\n",
    "    axes[i].fill_between(x_range, low[sort_idx], high[sort_idx], alpha=0.3, color=col)\n",
    "    axes[i].scatter(x_range, y_test_arr[sort_idx], s=1, alpha=0.3, color=\"black\")\n",
    "    axes[i].set_title(f\"{name} (Cov={cov:.3f}, Width={wid:.3f})\")\n",
    "\n",
    "# Hide the last empty subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table including all methods\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\n",
    "            \"MAPIE Split Conformal\",\n",
    "            \"MAPIE CV+ (Jackknife+)\",\n",
    "            \"MAPIE CQR\",\n",
    "            \"Perpetual Method 1 (Min-Max)\",\n",
    "            \"Perpetual Method 2 (GRP)\",\n",
    "            \"Perpetual Method 3 (Weight Var)\",\n",
    "            \"Perpetual Method 4 (Native)\",\n",
    "        ],\n",
    "        \"Coverage\": [\n",
    "            mapie_coverage,\n",
    "            m_cv_coverage,\n",
    "            m_cqr_coverage,\n",
    "            m1_coverage,\n",
    "            m2_coverage,\n",
    "            m3_wv_coverage,\n",
    "            m3_coverage,\n",
    "        ],\n",
    "        \"Mean Width\": [\n",
    "            mapie_width,\n",
    "            m_cv_width,\n",
    "            m_cqr_width,\n",
    "            m1_width,\n",
    "            m2_width,\n",
    "            m3_wv_width,\n",
    "            m3_width,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "summary[\"Width vs MAPIE (Split)\"] = summary[\"Mean Width\"] / mapie_width\n",
    "summary[\"Width vs MAPIE (Split)\"] = summary[\"Width vs MAPIE (Split)\"].map(\n",
    "    lambda x: f\"{x:.1%}\"\n",
    ")\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "\n",
    "Methods summary: All methods achieve requested coverage. Perpetual's methods and MAPIE CQR provide adaptive widths. \n",
    "The Weight Variance (M3) method provides highly competitive adaptive intervals using purely internal leaf statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
