{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install lightgbm optuna scikit-learn pandas matplotlib seaborn IProgress jupyter ipywidgets -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install ../../target/wheels/perpetual-1.0.0-cp313-cp313-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from time import process_time, time\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from perpetual import PerpetualBooster\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold, cross_validate, train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "data = fetch_openml(name=\"TVS_Loan_Default\", version=1, return_X_y=False, as_frame=True)\n",
    "X = data.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = X[X[\"V32\"] == 0]\n",
    "df_minority = X[X[\"V32\"] != 0]\n",
    "\n",
    "print(f\"Majority (zeros) size: {len(df_majority)}\")\n",
    "print(f\"Minority (non-zeros) size: {len(df_minority)}\")\n",
    "\n",
    "n_samples = len(df_minority) * 10\n",
    "\n",
    "df_majority_undersampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,  # Sample without replacement\n",
    "    n_samples=n_samples,  # Match number of samples in minority class\n",
    "    random_state=42,  # Set a seed for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Undersampled majority size: {len(df_majority_undersampled)}\")\n",
    "\n",
    "X = pd.concat([df_minority, df_majority_undersampled])\n",
    "\n",
    "print(\"\\nClass distribution in the balanced dataframe:\")\n",
    "print(X[\"V32\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.pop(\"V32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "X[object_cols] = X[object_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=[\"V16\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(seed):\n",
    "    scoring = \"neg_log_loss\"\n",
    "    metric_function = log_loss\n",
    "    metric_name = \"log_loss\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=seed\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        scoring,\n",
    "        metric_function,\n",
    "        metric_name,\n",
    "    )\n",
    "\n",
    "\n",
    "def objective_function(trial, seed, n_estimators, X_train, y_train, scoring):\n",
    "    params = {\n",
    "        \"seed\": seed,\n",
    "        \"verbosity\": -1,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.5, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 1e-6, 1.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-6, 1.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-6, 1.0, log=True),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 33),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 1024),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 100),\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "        return_train_score=True,\n",
    "        return_estimator=True,\n",
    "    )\n",
    "\n",
    "    return -1 * np.mean(cv_results[\"test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "n_trials = 100\n",
    "seed = 0\n",
    "\n",
    "(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    scoring,\n",
    "    metric_function,\n",
    "    metric_name,\n",
    ") = prepare_data(seed)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=seed)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "obj = partial(\n",
    "    objective_function,\n",
    "    seed=seed,\n",
    "    n_estimators=n_estimators,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring=scoring,\n",
    ")\n",
    "\n",
    "start = process_time()\n",
    "tick = time()\n",
    "study.optimize(obj, n_trials=n_trials)\n",
    "stop = process_time()\n",
    "\n",
    "\n",
    "print(f\"seed: {seed}, cpu time: {stop - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from perpetual.sklearn import PerpetualClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
    "\n",
    "params = study.best_trial.params\n",
    "params[\"n_estimators\"] = n_estimators\n",
    "params[\"seed\"] = seed\n",
    "params[\"verbosity\"] = -1\n",
    "perp = PerpetualClassifier(budget=1.5)\n",
    "lgbm = LGBMClassifier(**params)\n",
    "lgbm_isotonic = CalibratedClassifierCV(\n",
    "    LGBMClassifier(**params), cv=5, method=\"isotonic\"\n",
    ")\n",
    "lgbm_sigmoid = CalibratedClassifierCV(LGBMClassifier(**params), cv=5, method=\"sigmoid\")\n",
    "\n",
    "\n",
    "clf_list = [\n",
    "    (perp, \"Perpetual\"),\n",
    "    (lgbm, \"LightGBM\"),\n",
    "    (lgbm_isotonic, \"LightGBM + Isotonic\"),\n",
    "    (lgbm_sigmoid, \"LightGBM + Sigmoid\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = GridSpec(4, 2)\n",
    "colors = plt.get_cmap(\"Dark2\")\n",
    "\n",
    "ax_calibration_curve = fig.add_subplot(gs[:2, :2])\n",
    "calibration_displays = {}\n",
    "for i, (clf, name) in enumerate(clf_list):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "    display = CalibrationDisplay.from_predictions(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        n_bins=n_bins,\n",
    "        name=name,\n",
    "        ax=ax_calibration_curve,\n",
    "        color=colors(i),\n",
    "    )\n",
    "    calibration_displays[name] = display\n",
    "\n",
    "ax_calibration_curve.grid()\n",
    "ax_calibration_curve.set_title(\"Calibration plots (Naive Bayes)\")\n",
    "\n",
    "# Add histogram\n",
    "grid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\n",
    "for i, (_, name) in enumerate(clf_list):\n",
    "    row, col = grid_positions[i]\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "\n",
    "    ax.hist(\n",
    "        calibration_displays[name].y_prob,\n",
    "        range=(0, 1),\n",
    "        bins=n_bins,\n",
    "        label=name,\n",
    "        color=colors(i),\n",
    "    )\n",
    "    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "perp.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import plot_importance\n",
    "\n",
    "plot_importance(clf_list[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "y_pred = clf_list[0][0].predict(X_test)\n",
    "y_proba = clf_list[0][0].predict_proba(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_proba[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "\n",
    "def expected_calibration_error(\n",
    "    y_true: Union[np.ndarray, Sequence[int]],\n",
    "    y_pred: Union[np.ndarray, Sequence[float]],\n",
    "    n_bins: int = 10,\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates the Expected Calibration Error (ECE) for predicted probabilities.\n",
    "\n",
    "    ECE is the weighted average of the absolute difference between the mean\n",
    "    true outcome and the mean predicted probability in each confidence bin.\n",
    "    The weights are the proportion of samples falling into each bin.\n",
    "\n",
    "    Args:\n",
    "        y_true: True binary labels (0 or 1).\n",
    "        y_pred: Predicted probabilities for the positive class (between 0 and 1).\n",
    "        n_bins: The number of bins to use for the calibration curve (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The calculated ECE (float).\n",
    "        - The array of mean true probabilities per bin (prob_true).\n",
    "        - The array of mean predicted probabilities per bin (prob_pred).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    N = len(y_true)\n",
    "\n",
    "    # 1. Calculate the mean true probability and mean predicted probability for non-empty bins\n",
    "    # prob_true and prob_pred only contain values for non-empty bins.\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "\n",
    "    # 2. Determine the counts for the weights\n",
    "    # The bins are uniform over [0, 1].\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "\n",
    "    # Calculate the count of samples that fall into each of the n_bins.\n",
    "    # The range (0, 1) is used implicitly by calibration_curve.\n",
    "    counts, _ = np.histogram(y_pred, bins=bins, range=(0.0, 1.0))\n",
    "\n",
    "    # 3. Filter counts to match the non-empty bins returned by calibration_curve.\n",
    "    # The only bins that contribute to ECE are those that are not empty (count > 0).\n",
    "    non_empty_counts = counts[counts > 0]\n",
    "\n",
    "    # 4. Calculate the weights (Ni / N)\n",
    "    # The weight is the fraction of total samples in each non-empty bin.\n",
    "    weights = non_empty_counts / N\n",
    "\n",
    "    # 5. Calculate the ECE\n",
    "    # ECE = sum_i (Weight_i * |prob_true_i - prob_pred_i|)\n",
    "    ece = np.sum(weights * np.abs(prob_true - prob_pred))\n",
    "\n",
    "    return ece, prob_true, prob_pred, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    expected_calibration_error(\n",
    "        y_test, clf_list[0][0].predict_proba(X_test)[:, 1], n_bins\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    expected_calibration_error(\n",
    "        y_test, clf_list[1][0].predict_proba(X_test)[:, 1], n_bins\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    expected_calibration_error(\n",
    "        y_test, clf_list[2][0].predict_proba(X_test)[:, 1], n_bins\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    expected_calibration_error(\n",
    "        y_test, clf_list[3][0].predict_proba(X_test)[:, 1], n_bins\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_perp = perp.predict(X_train)\n",
    "y_proba_train_perp = perp.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_perp = perp.predict(X_test)\n",
    "y_proba_test_perp = perp.predict_proba(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_test_perp))\n",
    "print(f1_score(y_test, y_pred_test_perp))\n",
    "print(roc_auc_score(y_test, y_proba_test_perp[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expected_calibration_error(y_test, y_proba_test_perp[:, 1], n_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nodes_test = perp.predict_nodes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaf_nodes(perp: PerpetualBooster):\n",
    "    return [\n",
    "        {node.num: node for node in tree_nodes if node.is_leaf}\n",
    "        for tree_nodes in perp.get_node_lists()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(leaf_nodes, pred_nodes):\n",
    "    pred_weights = np.array(\n",
    "        [\n",
    "            [\n",
    "                [\n",
    "                    leaf_nodes[i][key].weights\n",
    "                    for key in leaf_nodes[i].keys() & set(nodes)\n",
    "                ][0]\n",
    "                for nodes in tree_nodes\n",
    "            ]\n",
    "            for i, tree_nodes in enumerate(pred_nodes)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return np.sort(pred_weights, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "print(bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "perp_models = []\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(X_train, y_train)):\n",
    "    print(f\"Fold {i}\")\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train], X_train.iloc[test]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train], y_train.iloc[test]\n",
    "    perp = PerpetualBooster(budget=1.0)\n",
    "    perp.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "    pred_weights = get_weights(get_leaf_nodes(perp), perp.predict_nodes(X_test_cv))\n",
    "    y_proba_cv = perp.predict_proba(X_test_cv)\n",
    "\n",
    "    bin_indices_cv = np.digitize(y_proba_cv[:, 1], bin_edges[1:-1])\n",
    "\n",
    "    cal_dicts = []\n",
    "\n",
    "    for j in range(n_bins):\n",
    "        bin_edge_left = bin_edges[j]\n",
    "        bin_edge_right = bin_edges[j + 1]\n",
    "\n",
    "        proba_true_test_bin = y_test_cv[bin_indices_cv == j]\n",
    "        count = len(proba_true_test_bin)\n",
    "        proba_true_test = np.mean(proba_true_test_bin)\n",
    "        proba_pred_test = np.mean(y_proba_cv[:, 1][bin_indices_cv == j])\n",
    "        for w_i in range(4):\n",
    "            weights_lower = np.sum(pred_weights[:, :, w_i], axis=0) + perp.base_score\n",
    "            weights_upper = (\n",
    "                np.sum(pred_weights[:, :, w_i + 1], axis=0) + perp.base_score\n",
    "            )\n",
    "            for k in range(11):\n",
    "                if k == 0:\n",
    "                    p_cal_prev = perp.base_score\n",
    "                    cal_weight_prev = 0.0\n",
    "                cal_weight = k / 10\n",
    "                w = weights_lower * (1 - cal_weight) + weights_upper * cal_weight\n",
    "                p = 1.0 / (1.0 + np.exp(-w))\n",
    "                p_bin = p[bin_indices_cv == j]\n",
    "                p_cal = np.mean(p_bin)\n",
    "                print(\n",
    "                    f\"Bin: {j}, count: {count}, w_i: {w_i}, cal_weight: {cal_weight:.1f}, proba_true: {proba_true_test}, proba_pred: {proba_pred_test}, p_cal: {p_cal}\"\n",
    "                )\n",
    "\n",
    "                if p_cal > proba_true_test:\n",
    "                    c_weight = np.interp(\n",
    "                        proba_true_test,\n",
    "                        [p_cal_prev, p_cal],\n",
    "                        [cal_weight_prev, cal_weight],\n",
    "                    )\n",
    "                    cal_dicts.append(\n",
    "                        {\n",
    "                            \"bin_edges\": (bin_edge_left, bin_edge_right),\n",
    "                            \"weight_indices\": (w_i, w_i + 1),\n",
    "                            \"cal_weight\": c_weight,\n",
    "                        }\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    p_cal_prev = p_cal\n",
    "                    cal_weight_prev = cal_weight\n",
    "\n",
    "            else:\n",
    "                continue  # only executed if the inner loop did NOT break\n",
    "            break  # only executed if the inner loop DID break\n",
    "\n",
    "        if count == 0:\n",
    "            print(f\"Warning: Bin {j} is empty.\")\n",
    "            cal_dicts.append(\n",
    "                {\n",
    "                    \"bin_edges\": (bin_edge_left, bin_edge_right),\n",
    "                    \"weight_indices\": (2, 2),\n",
    "                    \"cal_weight\": 1.0,\n",
    "                }\n",
    "            )\n",
    "        elif w_i == 3 and k == 10:\n",
    "            print(\n",
    "                f\"Warning: Could not calibrate bin {j} with proba_true_test {proba_true_test}\"\n",
    "            )\n",
    "            cal_dicts.append(\n",
    "                {\n",
    "                    \"bin_edges\": (bin_edge_left, bin_edge_right),\n",
    "                    \"weight_indices\": (w_i + 1, w_i + 1),\n",
    "                    \"cal_weight\": 1.0,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    print(cal_dicts)\n",
    "\n",
    "    perp_models.append((perp, cal_dicts))\n",
    "\n",
    "    ece, prob_true, prob_pred, weights = expected_calibration_error(\n",
    "        y_test_cv, y_proba_cv[:, 1], n_bins\n",
    "    )\n",
    "\n",
    "    print(f\"ECE: {ece}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, c_d in perp_models:\n",
    "    print(c_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dicts = [dicts for _, dicts in perp_models]\n",
    "weight_indices_left = [[e[\"weight_indices\"][0] for e in d] for d in c_dicts]\n",
    "print(weight_indices_left)\n",
    "weight_indices_left = np.median(np.array(weight_indices_left), axis=0).astype(int)\n",
    "print(weight_indices_left)\n",
    "weight_indices_right = [[e[\"weight_indices\"][1] for e in d] for d in c_dicts]\n",
    "print(weight_indices_right)\n",
    "weight_indices_right = np.median(np.array(weight_indices_right), axis=0).astype(int)\n",
    "print(weight_indices_right)\n",
    "cal_weights = [[e[\"cal_weight\"] for e in d] for d in c_dicts]\n",
    "print(cal_weights)\n",
    "cal_weights_median = np.median(np.array(cal_weights), axis=0)\n",
    "print(cal_weights_median)\n",
    "cal_weights_mean = np.mean(np.array(cal_weights), axis=0)\n",
    "print(cal_weights_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[e[\"weight_indices\"][0] for e in d] for d in c_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[e[\"weight_indices\"][1] for e in d] for d in c_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight_indices_left)\n",
    "print(weight_indices_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_indices_left[2] = 3\n",
    "weight_indices_right[2] = 4\n",
    "cal_weights_median[2] = 0.4\n",
    "weight_indices_left[3] = 4\n",
    "weight_indices_right[3] = 4\n",
    "weight_indices_left[4] = 4\n",
    "weight_indices_right[4] = 4\n",
    "weight_indices_left[5] = 2\n",
    "weight_indices_right[5] = 3\n",
    "cal_weights_median[5] = 0.1\n",
    "weight_indices_left[6] = 0\n",
    "weight_indices_right[6] = 1\n",
    "weight_indices_left[7] = 1\n",
    "weight_indices_right[7] = 1\n",
    "weight_indices_left[8] = 2\n",
    "weight_indices_right[8] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = []\n",
    "y_proba_cal = []\n",
    "\n",
    "for m, _ in perp_models:\n",
    "    y_pred = m.predict(X_test)\n",
    "    y_proba_model = m.predict_proba(X_test)[:, 1]\n",
    "    y_proba.append(y_proba_model)\n",
    "    pred_weights = get_weights(get_leaf_nodes(m), m.predict_nodes(X_test))\n",
    "    bin_indices = np.digitize(y_proba_model, bin_edges[1:-1])\n",
    "\n",
    "    y_proba_cal_model = []\n",
    "\n",
    "    for i, b_i in enumerate(bin_indices):\n",
    "        # bin_cal_dict = next((item for item in c_dict if item[\"bin_edges\"][0] <= y_proba_model[i] < item[\"bin_edges\"][1]), None)\n",
    "        # w_i_lower, w_i_upper = bin_cal_dict[\"weight_indices\"]\n",
    "        # cal_weight = bin_cal_dict[\"cal_weight\"]\n",
    "\n",
    "        w_i_lower, w_i_upper = weight_indices_left[b_i], weight_indices_right[b_i]\n",
    "        cal_weight = cal_weights_median[b_i]\n",
    "\n",
    "        weights_lower = np.sum(pred_weights[:, i, w_i_lower], axis=0) + m.base_score\n",
    "        weights_upper = np.sum(pred_weights[:, i, w_i_upper], axis=0) + m.base_score\n",
    "\n",
    "        w = weights_lower * (1 - cal_weight) + weights_upper * cal_weight\n",
    "        p_cal = 1.0 / (1.0 + np.exp(-w))\n",
    "        y_proba_cal_model.append(p_cal)\n",
    "\n",
    "    y_proba_cal.append(y_proba_cal_model)\n",
    "\n",
    "y_proba = np.mean(np.array(y_proba), axis=0)\n",
    "y_proba_cal = np.mean(np.array(y_proba_cal), axis=0)\n",
    "\n",
    "print(expected_calibration_error(y_test, y_proba, n_bins))\n",
    "print(expected_calibration_error(y_test, y_proba_cal, n_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.03398951 - 0.03106936) * 7.70118940e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(figsize=(8, 8))\n",
    "disp = CalibrationDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_proba,\n",
    "    n_bins=n_bins,\n",
    "    name=\"perp\",\n",
    "    ax=axis,\n",
    "    ref_line=True,\n",
    ")\n",
    "disp_cal = CalibrationDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_proba_cal,\n",
    "    n_bins=n_bins,\n",
    "    name=\"perp_cal\",\n",
    "    ax=axis,\n",
    "    ref_line=True,\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_cal(trial, models, X_train, y_train, cv):\n",
    "    y_proba = []\n",
    "    y_proba_cal = []\n",
    "    y_train_shuffled = []\n",
    "\n",
    "    weight_indices_cal = [\n",
    "        trial.suggest_float(f\"w_i_cal_{i}\", 1.0, 4.0) for i in range(n_bins)\n",
    "    ]\n",
    "\n",
    "    for i, (train, test) in enumerate(cv.split(X_train, y_train)):\n",
    "        _X_train_cv, X_test_cv = X_train.iloc[train], X_train.iloc[test]\n",
    "        _y_train_cv, y_test_cv = y_train.iloc[train], y_train.iloc[test]\n",
    "\n",
    "        y_train_shuffled.extend(list(y_test_cv))\n",
    "\n",
    "        m = models[i]\n",
    "\n",
    "        y_proba_model = m.predict_proba(X_test_cv)[:, 1]\n",
    "        y_proba.extend(list(y_proba_model))\n",
    "        pred_weights = get_weights(get_leaf_nodes(m), m.predict_nodes(X_test_cv))\n",
    "        bin_indices = np.digitize(y_proba_model, bin_edges[1:-1])\n",
    "\n",
    "        y_proba_cal_model = []\n",
    "\n",
    "        for i, b_i in enumerate(bin_indices):\n",
    "            cal_weight, w_i_lower = math.modf(weight_indices_cal[b_i])\n",
    "            w_i_lower = int(w_i_lower)\n",
    "            w_i_upper = w_i_lower + 1\n",
    "\n",
    "            weights_lower = np.sum(pred_weights[:, i, w_i_lower], axis=0) + m.base_score\n",
    "            weights_upper = np.sum(pred_weights[:, i, w_i_upper], axis=0) + m.base_score\n",
    "\n",
    "            w = weights_lower * (1 - cal_weight) + weights_upper * cal_weight\n",
    "            p_cal = 1.0 / (1.0 + np.exp(-w))\n",
    "            y_proba_cal_model.append(float(p_cal))\n",
    "\n",
    "        y_proba_cal.extend(list(y_proba_cal_model))\n",
    "\n",
    "    return expected_calibration_error(y_train_shuffled, y_proba_cal, n_bins)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cal = partial(\n",
    "    objective_cal,\n",
    "    models=[m for m, _ in perp_models],\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    cv=cv,\n",
    ")\n",
    "sampler_cal = optuna.samplers.TPESampler(seed=seed)\n",
    "study_cal = optuna.create_study(direction=\"minimize\", sampler=sampler_cal)\n",
    "study_cal.optimize(obj_cal, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = []\n",
    "y_proba_cal = []\n",
    "\n",
    "weight_indices_cal = [v for k, v in study_cal.best_trial.params.items()]\n",
    "print(weight_indices_cal)\n",
    "\n",
    "for m, _ in perp_models:\n",
    "    y_pred = m.predict(X_test)\n",
    "    y_proba_model = m.predict_proba(X_test)[:, 1]\n",
    "    y_proba.append(y_proba_model)\n",
    "    pred_weights = get_weights(get_leaf_nodes(m), m.predict_nodes(X_test))\n",
    "    bin_indices = np.digitize(y_proba_model, bin_edges[1:-1])\n",
    "\n",
    "    y_proba_cal_model = []\n",
    "\n",
    "    for i, b_i in enumerate(bin_indices):\n",
    "        cal_weight, w_i_lower = math.modf(weight_indices_cal[b_i])\n",
    "        w_i_lower = int(w_i_lower)\n",
    "        w_i_upper = w_i_lower + 1\n",
    "\n",
    "        weights_lower = np.sum(pred_weights[:, i, w_i_lower], axis=0) + m.base_score\n",
    "        weights_upper = np.sum(pred_weights[:, i, w_i_upper], axis=0) + m.base_score\n",
    "\n",
    "        w = weights_lower * (1 - cal_weight) + weights_upper * cal_weight\n",
    "\n",
    "        y_proba_cal_model.append(w)\n",
    "\n",
    "    y_proba_cal.append(y_proba_cal_model)\n",
    "\n",
    "y_proba = np.mean(np.array(y_proba), axis=0)\n",
    "y_proba_cal = np.mean(np.array(y_proba_cal), axis=0)\n",
    "y_proba_cal = 1.0 / (1.0 + np.exp(-y_proba_cal))\n",
    "\n",
    "print(expected_calibration_error(y_test, y_proba, n_bins))\n",
    "print(expected_calibration_error(y_test, y_proba_cal, n_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(figsize=(8, 8))\n",
    "disp = CalibrationDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_proba,\n",
    "    n_bins=n_bins,\n",
    "    name=\"perp\",\n",
    "    ax=axis,\n",
    "    ref_line=True,\n",
    ")\n",
    "disp_cal = CalibrationDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_proba_cal,\n",
    "    n_bins=n_bins,\n",
    "    name=\"perp_cal\",\n",
    "    ax=axis,\n",
    "    ref_line=True,\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ece, prob_true, prob_pred, weights = expected_calibration_error(\n",
    "    y_test_cv, y_proba_cv[:, 1], n_bins\n",
    ")\n",
    "\n",
    "print(f\"ECE: {ece}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-1, 0.2, 6.4, 3.0, 1.6])\n",
    "bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])\n",
    "inds = np.digitize(x, bins)\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_indices_train = np.digitize(y_proba_train_perp[:, 1], bin_edges[1:-1])\n",
    "print(bin_indices_train)\n",
    "print(min(bin_indices_train))\n",
    "print(max(bin_indices_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_true_train = np.mean(y_train[bin_indices_train == 0])\n",
    "print(len(y_train[bin_indices_train == 0]))\n",
    "print(proba_true_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred_train = np.mean(y_proba_train_perp[:, 1][bin_indices_train == 0])\n",
    "print(len(y_proba_train_perp[:, 1][bin_indices_train == 0]))\n",
    "print(proba_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_weights = get_weights(get_leaf_nodes(perp), perp.predict_nodes(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_indices_test = np.digitize(y_proba_test_perp[:, 1], bin_edges[1:-1])\n",
    "print(bin_indices_test)\n",
    "print(min(bin_indices_test))\n",
    "print(max(bin_indices_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_true_test = np.mean(y_test[bin_indices_test == 0])\n",
    "print(len(y_test[bin_indices_test == 0]))\n",
    "print(proba_true_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred_test = np.mean(y_proba_test_perp[:, 1][bin_indices_test == 0])\n",
    "print(len(y_proba_test_perp[:, 1][bin_indices_test == 0]))\n",
    "print(proba_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_weights = get_weights(get_leaf_nodes(perp), pred_nodes_test)\n",
    "pred_lower = np.sum(np.min(pred_weights, axis=2), axis=0) + perp.base_score\n",
    "pred_lower = 1.0 / (1.0 + np.exp(-pred_lower))\n",
    "pred_lower.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(pred_lower[bin_indices_test == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_weights = get_weights(get_leaf_nodes(perp), pred_nodes_test)\n",
    "pred_upper = np.sum(np.max(pred_weights, axis=2), axis=0) + perp.base_score\n",
    "pred_upper = 1.0 / (1.0 + np.exp(-pred_upper))\n",
    "pred_upper.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.displot(pred_upper - pred_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(pred_lower, pred_upper, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(pred_upper - pred_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(\n",
    "    low=0, high=5, size=(pred_weights.shape[0], pred_weights.shape[1], 100)\n",
    ")\n",
    "new_pred_weights = np.take_along_axis(pred_weights, indices, axis=2)\n",
    "print(f\"New array shape: {new_pred_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_weights_sum = np.sum(new_pred_weights, axis=0) + perp.base_score\n",
    "new_pred_weights_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(new_pred_weights_sum[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_weights_sum_proba = 1.0 / (1.0 + np.exp(-new_pred_weights_sum))\n",
    "new_pred_weights_sum_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(new_pred_weights_sum_proba[11000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    np.max(new_pred_weights_sum_proba, axis=1)\n",
    "    - np.min(new_pred_weights_sum_proba, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forust-main-perp-oss (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
